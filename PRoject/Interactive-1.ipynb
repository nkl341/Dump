{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to base (Python 3.11.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m/Users/coltonvandenburg/Downloads/vandenburg_demo_tsne_irisData_v1.py:27\u001b[0m\n\u001b[1;32m     23\u001b[0m date_c \u001b[39m=\u001b[39m date_o\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m script_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m__file__\u001b[39m))     \u001b[39m# Get the directory of the current script\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m csv_file_path, programName_c \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(script_dir, \u001b[39m'\u001b[39m\u001b[39mirisData_modified.csv\u001b[39m\u001b[39m'\u001b[39m)   \u001b[39m# Construct the full path to the CSV file\u001b[39;00m\n\u001b[1;32m     29\u001b[0m irisData_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(csv_file_path)    \u001b[39m# Load the CSV file\u001b[39;00m\n\u001b[1;32m     31\u001b[0m ix \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mfind(programName_c,\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# program\t\tvandenburg_demo_tsne_irisData_v1.py\n",
    "# purpose\t    Demonstrate perceptron with iris data\n",
    "# usage         script\n",
    "# notes         (1)\n",
    "# date\t\t\t2/14/2024\n",
    "# programmer   Colton Vandenburg\n",
    "\n",
    "import datetime          # used for getting the date\n",
    "import os                # used for getting the basic file name (returns lower case)\n",
    "from sklearn.manifold import TSNE as tsne   # t-distributed stochastic neighbor embedding\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing    # Import label encoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# ============== COMMON INITIALIZATION =====================\n",
    "date_o = datetime.datetime.today()\n",
    "date_c = date_o.strftime('%m/%d/%Y')\n",
    "\n",
    "\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))     # Get the directory of the current script\n",
    "csv_file_path, programName_c = os.path.join(script_dir, 'irisData_modified.csv')   # Construct the full path to the CSV file\n",
    "\n",
    "irisData_df = pd.read_csv(csv_file_path)    # Load the CSV file\n",
    "\n",
    "ix = str.find(programName_c,'.')\n",
    "\n",
    "fileName_c = 'irisData_modified.csv'\n",
    "programMsg_c = programName_c + ' (' + date_c + ')'\n",
    "\n",
    "\n",
    "authorName_c = 'Colton Vandenburg'\n",
    "figName_c = programName_c[:ix]+'_fig.png'\n",
    "\n",
    "\n",
    "# ========== get data frame, split, get numerical data ==============\n",
    "irisData_df = pd.read_csv((fileName_c))\n",
    "irisData_featureNames_cv = irisData_df.columns\n",
    "irisData_featureNames_cv = irisData_featureNames_cv[:4]     # remove type column\n",
    "\n",
    "tts_ts = 0.3\n",
    "tts_rs = 24     #26 performs poorly - why?\n",
    "irisDataT_df, irisDataV_df = train_test_split(irisData_df,test_size = tts_ts, random_state=tts_rs) \n",
    "irisDataT_num_df = irisDataT_df.drop(\"type\", axis = 1)    # numerical data only\n",
    "\n",
    "# ============== get labels, preprocess train data except for scaling  ======================\n",
    "irisDataT_labels = irisDataT_df[\"type\"]\n",
    "label_encoder = preprocessing.LabelEncoder()  \n",
    "irisDataT_labels_v = label_encoder.fit_transform(irisDataT_labels)\n",
    "\n",
    "imputer = SimpleImputer(strategy = \"median\")\n",
    "imputer.fit(irisDataT_num_df)\n",
    "irisDataT_num_df = imputer.transform(irisDataT_num_df)\n",
    "\n",
    "# ================= get labels, preprocess verify data ==================\n",
    "irisDataV_num_df = irisDataV_df.drop(\"type\", axis = 1)    # numerical data only\n",
    "irisDataV_labels = irisDataV_df[\"type\"]\n",
    "irisDataV_labels_v = label_encoder.fit_transform(irisDataV_labels) \n",
    "\n",
    "irisDataV_num_df = imputer.transform(irisDataV_num_df)\n",
    "\n",
    "# ================ perceptron, non-scaled ===================\n",
    "ppn_eta = .01\n",
    "ppn_rs = 1\n",
    "ppn_T = Perceptron(eta0 = ppn_eta, random_state = ppn_rs)    # what is best value for eta?\n",
    "ppn_T.fit(irisDataT_num_df, irisDataT_labels_v)\n",
    "\n",
    "predict_T = ppn_T.predict(irisDataT_num_df)         # try on test data (should be better than verify)\n",
    "accuracy_T = accuracy_score(irisDataT_labels_v, predict_T)\n",
    "predict_V = ppn_T.predict(irisDataV_num_df)         # now do on verify data\n",
    "accuracy_V = accuracy_score(irisDataV_labels_v, predict_V)\n",
    "\n",
    "print('accuracy_T = ',accuracy_T)\n",
    "print('accuracy_V = ',accuracy_V)\n",
    "\n",
    "# ================ perceptron, standardized ===================\n",
    "sc = StandardScaler()       # 0-mean, sigma=1; performs better than minMaxScaler(), which does [0,1]\n",
    "sc.fit(irisDataT_num_df)    # do the fit step on the training data\n",
    "irisDataTs_num_df = sc.transform(irisDataT_num_df)  # apply transform on train data\n",
    "irisDataVs_num_df = sc.transform(irisDataV_num_df)  # do same transform on verify data\n",
    "\n",
    "ppn_T.fit(irisDataTs_num_df, irisDataT_labels_v)\n",
    "\n",
    "predict_Ts = ppn_T.predict(irisDataTs_num_df)         # try on test data (should be better than verify)\n",
    "accuracy_Ts = accuracy_score(irisDataT_labels_v, predict_Ts)\n",
    "predict_Vs = ppn_T.predict(irisDataVs_num_df)         # now do on verify data\n",
    "accuracy_Vs = accuracy_score(irisDataV_labels_v, predict_Vs)\n",
    "\n",
    "print('accuracy_Ts = ',accuracy_Ts)\n",
    "print('accuracy_Vs = ',accuracy_Vs)\n",
    "\n",
    "# ================= perceptron, normalized ==================\n",
    "scn = MinMaxScaler()        # does not do as well as StandardScaler\n",
    "scn.fit(irisDataT_num_df)    # do the fit step on the training data\n",
    "irisDataTn_num_df = scn.transform(irisDataT_num_df)  # apply transform on train data\n",
    "irisDataVn_num_df = scn.transform(irisDataV_num_df)  # do same transform on verify data\n",
    "\n",
    "ppn_T.fit(irisDataTn_num_df, irisDataT_labels_v)\n",
    "\n",
    "predict_Tn = ppn_T.predict(irisDataTn_num_df)         # try on test data (should be better than verify)\n",
    "accuracy_Tn = accuracy_score(irisDataT_labels_v, predict_Tn)\n",
    "predict_Vn = ppn_T.predict(irisDataVn_num_df)         # now do on verify data\n",
    "accuracy_Vn = accuracy_score(irisDataV_labels_v, predict_Vn)\n",
    "\n",
    "print('accuracy_Tn = ',accuracy_Tn)\n",
    "print('accuracy_Vn = ',accuracy_Vn)\n",
    "\n",
    "# ===================== prepare to plot results ===================\n",
    "msg_tts_c = \"test_train_split: tts_ts = \" + \"%1.2f\" %(tts_ts) + \"; tts_rs = \" + str(tts_rs)\n",
    "msg_ppn_c = \"perceptron: ppn_eta = \" + \"%1.2f\" %(ppn_eta) + \"; ppn_rs = \" + str(ppn_rs)\n",
    "msg_plot_c = msg_tts_c + '; ' + msg_ppn_c\n",
    "\n",
    "msg_acc1_c = \"Accuracy: T = \" + \"%1.2f\" %(accuracy_T) + \"; V = \" + \"%1.2f\" %(accuracy_V)\n",
    "msg_acc2_c = \"Ts = \" + \"%1.2f\" %(accuracy_Ts) + \"; Vs = \" + \"%1.2f\" %(accuracy_Vs)\n",
    "msg_acc3_c = \"Tn = \" + \"%1.2f\" %(accuracy_Tn) + \"; Vs = \" + \"%1.2f\" %(accuracy_Vn)\n",
    "\n",
    "msg_acc_c = msg_acc1_c + '; ' + msg_acc2_c + '; ' + msg_acc3_c\n",
    "\n",
    "# ======================== PLOT RESULTS ======================\n",
    "#plt.figure(num=1, figsize=(11.2, 5.2))        # not consistent with boxplot\n",
    "plt.rcParams.update({'font.size': 9})           # 8-point fits a little better but still overlaps\n",
    "fig, axes = plt.subplots(2, 2, figsize=(11.2, 5.2))\n",
    "fig.suptitle(msg_acc_c,fontsize=9)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "sns.set(font_scale = 0.7)   # make fonts a little smaller on the box plot\n",
    "for kp, feature in enumerate(irisData_featureNames_cv):\n",
    "    sns.boxplot(x='type', y=feature, hue='type', data=irisData_df, ax=axes[kp//2, kp%2], palette='pastel')\n",
    "\n",
    "\n",
    "# ============= Make the subplots look a little nicer ================= \n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.15, wspace=0.2, hspace=0.3)\n",
    "\n",
    "# ================= label plot edges ==================\n",
    "plt.subplot(position=[0.0500,    0.93,    0.02500,    0.02500]) # U-left\n",
    "plt.axis('off')\n",
    "plt.text(0,.5, programMsg_c, fontsize=8)\n",
    "\n",
    "plt.subplot(position=[0.550,    0.93,    0.02500,    0.02500]) # U-right\n",
    "plt.axis('off')\n",
    "plt.text(0,.5, authorName_c, fontsize=8)\n",
    "\n",
    "plt.subplot(position=[0.0500,    0.02,    0.02500,    0.02500]) # L-left\n",
    "plt.axis('off')\n",
    "plt.text(0,.5, fileName_c, fontsize=8)\n",
    "\n",
    "plt.subplot(position=[0.3500,    0.02,    0.02500,    0.02500]) # L-right\n",
    "plt.axis('off')\n",
    "plt.text(0,.5, msg_plot_c, fontsize=8)\n",
    "\n",
    "\n",
    "plt.savefig(figName_c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'conversation.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m/nkl341/EE497_ML/Project/data_preprocess.py:51\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m content\n\u001b[1;32m     50\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mconversation.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 51\u001b[0m content \u001b[39m=\u001b[39m read_text_file(filename)\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_conversations\u001b[39m(conversations):\n\u001b[1;32m     54\u001b[0m     processed_conversations \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m/nkl341/EE497_ML/Project/data_preprocess.py:44\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_text_file\u001b[39m(filename):\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     45\u001b[0m         content \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     46\u001b[0m     content \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m content]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'conversation.txt'"
     ]
    }
   ],
   "source": [
    "# program\t\tdata_preprocess.py\n",
    "# purpose\t    Proprocess and standardize for training data\n",
    "# usage         write_dialogue_to_file('filename')\n",
    "#               read_text_file('filename')\n",
    "# notes         (1) \n",
    "# date\t\t\t2/7/2024\n",
    "# programmer    Colton Vandenburg\n",
    "import json\n",
    "import string\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def write_dialogue_to_file(filename):\n",
    "    filename = f\"{filename}_{datetime.date.today()}.txt\"\n",
    "    with open(filename, 'w') as file:\n",
    "        while True:\n",
    "            myWords = input(\"Enter dialogue for Me (or type 'exit' to quit): \")\n",
    "            if myWords.lower() == 'exit':\n",
    "                break\n",
    "            file.write(f\"Speaker 1: {myWords}\\n\")\n",
    "            \n",
    "            friend_words = input(\"Enter dialogue for Friend (or type 'exit' to quit): \")\n",
    "            if friend_words.lower() == 'exit':\n",
    "                if myWords.lower() == 'exit':\n",
    "                    return\n",
    "                break\n",
    "            file.write(f\"Speaker 2: {friend_words}\\n\")\n",
    "\n",
    "\n",
    "#write_dialogue_to_file('conversation')\n",
    "\n",
    "\n",
    "\n",
    "def read_text_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.readlines()\n",
    "    content = [line.encode('ascii', 'ignore').decode('ascii') for line in content]\n",
    "    return content\n",
    "\n",
    "\n",
    "filename = 'conversation.txt'\n",
    "content = read_text_file(filename)\n",
    "\n",
    "def preprocess_conversations(conversations):\n",
    "    processed_conversations = []\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for conversation in conversations:\n",
    "        processed_conversation = []\n",
    "        for utterance in conversation:\n",
    "            # Tokenization, Lowercasing, Remove Punctuation, Remove Empty Tokens\n",
    "            tokens = [token.lower().translate(str.maketrans('', '', string.punctuation)) for token in utterance.split() if token]\n",
    "            processed_conversation.append(' '.join(tokens))\n",
    "        processed_conversations.append(processed_conversation)\n",
    "\n",
    "    # Vectorize the labels using LabelEncoder\n",
    "    encoded_labels = label_encoder.fit_transform(range(len(processed_conversations)))\n",
    "\n",
    "    return processed_conversations, encoded_labels\n",
    "processed_content = preprocess_conversations(content)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on your preprocessed content\n",
    "vectorizer.fit(processed_content)\n",
    "\n",
    "# Transform the preprocessed content into vectors\n",
    "vectorized_content = vectorizer.transform(processed_content)\n",
    "\n",
    "# Now you can use the vectorized content for your machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nkl341/EE497_ML/Project/conversation.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m/nkl341/EE497_ML/Project/data_preprocess.py:51\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m content\n\u001b[1;32m     50\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnkl341/EE497_ML/Project/conversation.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 51\u001b[0m content \u001b[39m=\u001b[39m read_text_file(filename)\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_conversations\u001b[39m(conversations):\n\u001b[1;32m     54\u001b[0m     processed_conversations \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m/nkl341/EE497_ML/Project/data_preprocess.py:44\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_text_file\u001b[39m(filename):\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     45\u001b[0m         content \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     46\u001b[0m     content \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m content]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nkl341/EE497_ML/Project/conversation.txt'"
     ]
    }
   ],
   "source": [
    "# program\t\tdata_preprocess.py\n",
    "# purpose\t    Proprocess and standardize for training data\n",
    "# usage         write_dialogue_to_file('filename')\n",
    "#               read_text_file('filename')\n",
    "# notes         (1) \n",
    "# date\t\t\t2/7/2024\n",
    "# programmer    Colton Vandenburg\n",
    "import json\n",
    "import string\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def write_dialogue_to_file(filename):\n",
    "    filename = f\"{filename}_{datetime.date.today()}.txt\"\n",
    "    with open(filename, 'w') as file:\n",
    "        while True:\n",
    "            myWords = input(\"Enter dialogue for Me (or type 'exit' to quit): \")\n",
    "            if myWords.lower() == 'exit':\n",
    "                break\n",
    "            file.write(f\"Speaker 1: {myWords}\\n\")\n",
    "            \n",
    "            friend_words = input(\"Enter dialogue for Friend (or type 'exit' to quit): \")\n",
    "            if friend_words.lower() == 'exit':\n",
    "                if myWords.lower() == 'exit':\n",
    "                    return\n",
    "                break\n",
    "            file.write(f\"Speaker 2: {friend_words}\\n\")\n",
    "\n",
    "\n",
    "#write_dialogue_to_file('conversation')\n",
    "\n",
    "\n",
    "\n",
    "def read_text_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.readlines()\n",
    "    content = [line.encode('ascii', 'ignore').decode('ascii') for line in content]\n",
    "    return content\n",
    "\n",
    "\n",
    "filename = 'nkl341/EE497_ML/Project/conversation.txt'\n",
    "content = read_text_file(filename)\n",
    "\n",
    "def preprocess_conversations(conversations):\n",
    "    processed_conversations = []\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for conversation in conversations:\n",
    "        processed_conversation = []\n",
    "        for utterance in conversation:\n",
    "            # Tokenization, Lowercasing, Remove Punctuation, Remove Empty Tokens\n",
    "            tokens = [token.lower().translate(str.maketrans('', '', string.punctuation)) for token in utterance.split() if token]\n",
    "            processed_conversation.append(' '.join(tokens))\n",
    "        processed_conversations.append(processed_conversation)\n",
    "\n",
    "    # Vectorize the labels using LabelEncoder\n",
    "    encoded_labels = label_encoder.fit_transform(range(len(processed_conversations)))\n",
    "\n",
    "    return processed_conversations, encoded_labels\n",
    "processed_content = preprocess_conversations(content)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on your preprocessed content\n",
    "vectorizer.fit(processed_content)\n",
    "\n",
    "# Transform the preprocessed content into vectors\n",
    "vectorized_content = vectorizer.transform(processed_content)\n",
    "\n",
    "# Now you can use the vectorized content for your machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Project/conversation.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m content \u001b[39m=\u001b[39m read_text_file(\u001b[39m'\u001b[39m\u001b[39mProject/conversation.txt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mread_text_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_text_file\u001b[39m(filename):\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     45\u001b[0m         content \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     46\u001b[0m     content \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m content]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Project/conversation.txt'"
     ]
    }
   ],
   "source": [
    "content = read_text_file('Project/conversation.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
